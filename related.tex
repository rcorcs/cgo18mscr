\section{Related Work}\label{sec:relatedwork}

\subsection{{\IterComp}}
Various adaptive compilation techniques have been proposed for finding near optimal compilation
settings~\cite{agakov06,kulkarni04,stephenson03}. These approaches evaluate the performance of candidate compiler options through repeated
executions using the same input. All these approaches implicitly assume the found compiler settings work for other unseen datasets.
Chen~\etal~\cite{chen10,chen12a} show that this assumption is not true, and the best optimization options can differ across inputs. Still,
in most cases it is possible to find a combination of optimizations that produces good results across all inputs, by using a few tens of
inputs to evaluate our optimizations on. While a positive result, this is still impractical. Producing tens of realistic input sets might
be difficult. Even if not, each combination of optimization options has to be evaluated on every input set, increasing the time \itercomp
takes by at least an order of magnitude.

Recent research~\cite{chen12b,fang15} has used real data for performing online {\itercomp} on distributed data center applications. Each
worker receives a subset of the input dataset to evaluate a small set of optimization settings on. The best such setting from each round
is used for subsequent executions of the same code. By testing new settings and re-evaluating old ones on new datasets this approach makes
it more likely to select an optimization setting that works well across inputs. Like in~\cite{chen10} every setting is tested on the same
set of inputs, wasting time and energy. This approach only works well with MapReduce-like workloads, since it relies on the MapReduce
framework for repeating the same computation multiple times without side-effects or corrupting the input.

Fursin~\etal~\cite{fursin07} attempted to find a more general approach for online \itercomp. They proposed instructions per cycle (IPC) as
a metric for comparing runs of the same program with different optimization settings and different inputs: application binaries displaying
higher IPC are more efficient than lower IPC binaries. Their results show that IPC seems promising as a robust metric for {\itercomp}.
However, some optimization techniques affect IPC in the opposite way, making the program more efficient while lowering IPC. In the context
of multithreaded application, IPC cannot be used to estimate performance~\cite{alameldeen06,eyerman08}. Aalameldeen and
Wood~\cite{alameldeen06} suggest a work metric for quantifying efficiency, but choosing a unit of work is program specific and challenging.

%\subsection{Work and Input Size Metrics}

%Previous work have proposed profiling-based mechanism to estimate input sizes~\cite{zaparanuks12,coppa14}.
%Coppa~\etal~\cite{coppa14} in particular propose the concept of \textit{read memory size} for automatically estimating the size of the input passed to a routine, where \textit{read memory size} represents the number of distinct memory cells first accessed by a read operation.
%In other words, the \textit{read memory size} metric measures the size of the useful portion of the input's memory footprint.
%However, because we are interested in the amount of computational work performed in respect of a given input, the memory footprint of the input may not always have a direct correspondence to  the amount of computational work.

%Goldsmith~\etal~\cite{goldsmith07} use \textit{block frequency} as the measure for performance for empirically describing the asymptotic behavior of programs, which is known as empirical computational complexity.
%Block frequency is a relative metric that represents the number of times a basic block executes~\cite{ball94,ball96}.
%They argue in favor of block frequency due to its portability, repeatability and exactness, since it does not suffer from timer resolution problems or non-deterministic noises.
%Block frequency also has the advantage of being efficiently profiled by means of automatic code instrumentation~\cite{knuth73,ball94}.

%However, in the context of comparing different optimizations, although block frequency would be able to capture aspects of optimizations that simplify the control-flow graph (CFG), measuring work at the basic block resolution would not capture effects of optimizations at the instruction level.
%Because of that, we extend the idea of using basic block frequency to measure computational work by also considering the computational cost of each basic block.
%The computational cost of a basic block is given by weighing the instructions that it contains.

\subsection{Optimal Instrumentation}

Donald Knuth~\cite{knuth73} introduced an optimal algorithm for profiling block frequency, inserting much fewer probes than na\"ive
instrumentation approaches that were used in practice~\cite{knuth71}. Forman~\cite{forman81} proposed further overhead reductions by
placing the probes in basic blocks that are less likely to be executed based on static heuristics. Ball and Larus~\cite{ball94} offered a
detailed discussion comparing two approaches for optimally profiling block frequency, namely, placing probes on the vertices or the edges
of the control-flow graph (CFG). They show that the edge-based approach produces optimal placement of probes. Theirs is the algorithm we
used for our initial selection of probes before relaxation.
