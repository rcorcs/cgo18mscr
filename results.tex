\section{Experimental Results}

%In this section we discuss our experimental evaluation. Afterwards, we discuss our results concerning the instrumentations for profiling
%our work metric. Finally we present the results of the online {\itercomp}.

\subsection{Evaluation of the Instrumentation}

In this experiment we evaluate the runtime overhead introduced by the work efficiency profiling. We compile the program using the
{\flagstype -O3}. We then measure the wall-clock time of the instrumented and the native versions to calculate the runtime overhead. For
each benchmark, we compute the average overhead over all its 1,000 inputs.

Figure~\ref{fig:overhead-O3} shows the runtime overhead imposed by the work profiling using each of the instrumentation strategies over
their non-instrumented counterparts. The WPO relaxation was performed using profiling information for every specific input, which provides
the perfect information to perform the WPO relaxation. This means that the experiments show its best performance enabled by having perfect
profiling information.

%\begin{figure}[t]
%    \centering
%    \includegraphics[width=0.395\textwidth]{figs/adpcm_d-cfg-instr.pdf}
%    \caption{CFG of the function that contains the hot loop of the {\flagstype adpcm\_d} benchmark.}
%    \label{fig:adpcm_d-cfg-instr}
%\end{figure}

While the optimal profiling has a maximum overhead of near 60\%, the work profiling with a relaxation of 5\% threshold has an overhead of
less than 16\% for all benchmarks, with the WPO relaxation reaching an average overhead of only 5\%, compared to an average of 12\% for the
optimal profiling.

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figs/overhead-O3.pdf}
    \caption{Overhead of the instrumentations compiled with {\flagstype -O3}.}
    \label{fig:overhead-O3}
\end{figure*}
\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figs/error-O3.pdf}
    \caption{Dynamic error of the work profiling averaged over the 1000 inputs, after relaxing the number of probes.}
    \label{fig:error-O3}
\end{figure*}

The most critical case, amongst the evaluated benchmarks, happens with the
\texttt{adpcm\_d} program, with an overhead of about 59\% for the optimal
instrumentation.
With a threshold of only 5\%, the relaxation reduces
the overhead by a factor of $4.5\times$ over the optimal instrumentation,
and the WPO relaxation by a factor of $4.8\times$.
This benchmark has a single hot function
which consists mainly of a single hot loop with several branches inside it. %, as depicted by Figure~\ref{fig:adpcm_d-cfg-instr}.
Most of the improvement achieved by both strategies comes from removing only two probes from the hot loop. The two relaxed probes were
placed in branches with a high probability of being taken (above 55\% probability), but with a small contribution to the total amount of
work measured in DAG for this loop (both probes with an static error of about 1.3\%). Because these probes were placed in basic blocks with
a considerable execution frequency, removing the probes resulted in a significant reduction in runtime overhead.

A few of the benchmarks present abnormal regressions in their execution time.
This increase in the profiling overhead is counter-intuitive because the
relaxation algorithms only reduces the number of instrumented probes,
without changing their placement.
We suggest some reasons for these abnormal regressions.
By analyzing these cases, we noticed that several of the frequently executed
probes could not be removed by the relaxation strategies with small thresholds.
Another important aspect to point out is that we also observed
differences among the final CFGs after applying the \texttt{-O3} optimization.
Although relaxation does not directly alter the CFG, it is worth mentioning that
many optimization passes make use of LLVM's built-in target-based cost model,
which means that adding or removing instructions from basic blocks may affect
decisions during transformations of the code.
For example, some of the optimizations that make use of this built-in cost model
and also alter the CFG are: loop-unrolling, function inlining, and
simplifications of the CFG.
These changes in the program's CFGs may affect other compiler optimizations and
also possibly in hardware performance, for example affecting the instruction
caching.

Finally, as shown in Figure~\ref{fig:error-O3}, these improvements can be
obtained while incurring very little dynamic error.
This result confirms the overly conservative aspect of the relaxation algorithm
applied per DAG, while the WPO relaxation is significantly less conservative.
Although the error bound is not guaranteed by the WPO relaxation,
its dynamic error was always below the 5\% thresholds due to the use
of perfect profiling information.

\subsection{Evaluation of Online {\IterComp}}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figs/speedups.pdf}
    \caption{Speedups obtained from the final optimization sequence selected by the online {\itercomp}.
	         The speedups reported for each benchmark represents the average speedup across their complete 1000 input datasets.}
    \label{fig:speedups}
\end{figure*}

For comparison, we use five configurations.
In all configurations, the same optimization sequence is used for multiple inputs, using a dynamic input-window size, as explained in Section~\ref{sec:oic-infra}.
The average performance over the input window provides an estimate for the overall performance of the optimization sequence across distinct inputs.
The optimization sequences are ranked based on their average performance, and the best optimization sequence is selected.
%When selecting the best optimization sequence, they are ranked by the average of their performance measurement, using the WP metric, except for the Oracle-RM which uses the real speedup over \texttt{-O0}.
\begin{itemize}[leftmargin=3mm]
\item \textbf{Oracle-RM} executes the program twice for each input, measuring the real speedup for each optimization sequence, and then uses the real speedup over {\flagstype -O0} for guiding online {\itercomp}. This oracle provides a reference results for assessing the effectiveness work-efficiency metric.
\item \textbf{Oracle-PP} represents an oracle with a \textit{perfect} non-intrusive profiling.
  It uses the work-efficiency metric for guiding online {\itercomp}.
  Because this version simulates a \textit{perfect} profiling, with zero overhead,
  it allows us isolate and evaluate the effectiveness work-efficiency metric.
\item \textbf{Real-OP} corresponds to the online {\itercomp} as it would be applied in a real-world online scenario.
  The program is executed only once with each input.
  It uses the optimal work profiling for computing the work efficiency.
\item \textbf{Real-R5} is similar to the {Real-OP}, but with a relaxation of 5\% in the work profiling.
\item \textbf{Real-WPO-R5} is similar to the {Real-OP}, but with a whole-program relaxation of 5\% in the work profiling.
%\item \textbf{Real-IPC} uses IPC for guiding the online {\itercomp}.
\end{itemize}

The comparison between Oracle-RM and Oracle-PP is crucial for validating the
effectiveness of the work efficiency metric in guiding online {\itercomp},
while the other configurations demonstrate the viability of applying online
{\itercomp} in real-world scenarios.
For the relaxation strategies, we evaluate only those with a 5\% threshold, since
they offered better overhead reductions for little dynamic errors in the measurement.

%Figure~\ref{fig:window-size} shows the average input-window size for each benchmark and configuration.
%It shows that we can have a statistically sound measurement of the performance metric using just a small number of inputs.

%\begin{figure*}[t]
%    \centering
%    \includegraphics[width=\textwidth]{figs/window-size.pdf}
%    \caption{Average input-window sizes observed during the online {\itercomp}.}
%    \label{fig:window-size}
%\end{figure*}

\subsection{Performance Evaluation}

In order to evaluate the quality of the final optimization sequences selected by
each configuration of the online {\itercomp}, we compare their speedup over the
\texttt{-O3} optimization across all the 1,000 inputs of the benchmark being
optimized.
None of the configurations degrades the performance over \texttt{-O3} (with
statistical significance).

%When measuring the wall-clock time for each input, to reduce noise, we execute
%the same input until we have a statistically sound measurement, i.e. we execute
%until we have an interval no larger than 1\% with 99\% confidence.
Figure~\ref{fig:speedups} shows these average speedups for all test benchmarks.
This figure shows that the best optimization sequence selected with the Oracle-PP
is very close to the performance of the best optimization sequence selected with
the Oracle-RM, where Oracle-PP achieves on average about 80\% of the performance
improvement obtained by the Oracle-RM.
This result is important for demonstrating that our work efficiency metric has
the potential to produce good results in real-world online scenarios.

The Real-OP achieves 4\% improvement on average, which represents 45\% of the
performance improvement obtained by the Oracle-RM.
This difference to Oracle-PP is explained by the real (intrusive) profiling used
in the Real-OP configuration.
The profiling affects the search in two key ways:
\textit{(i.)} the overhead incurred by the profiling affects the execution time,
and therefore also affects the work efficiency metric,
\textit{(ii.)} the instrumentation code may affect many of the optimizations, e.g.,
due to the use of a global variable or in decisions taken based on a cost-model
for the instructions.
However, even in the Real-OP configuration, which has a highly intrusive instrumentation,
it is still possible to obtain performance improvements in a realistic online
{\itercomp}.

Finally, the evaluation also indicates that the use of relaxation algorithms
is beneficial not only for its reduction in overhead, which is directly observed
by the user, by it also tends to improve the quality of the search.
This improvement has to do with the aforementioned effects of an intrusive
instrumentation on the search.
The relaxation has the potential to reduce both effects of the profiling.
Both the Real-R5 and Real-WPO-R5 achieve about 60\% of the
performance improvement obtained by the Oracle-RM.
