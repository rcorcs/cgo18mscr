\section{Work Profiling}\label{sec:prof}

In this section, we describe how to measure work online. Total work is just the sum of the costs of all executed basic blocks, so it is
possible to embed its computation into the execution of the program. A na\"{\i}ve instrumentation would use a global counter initialized
to zero. Each basic block would then have only to increment this counter with the total cost of its
instructions every time it is executed. Although easy to implement, the na\"{\i}ve approach introduces a significant overhead.

Previous work on basic block profiling has proposed optimal ways for instrumenting the code with as little overhead as possible without
losing any profiling information~\citep{knuth73,ball94}. The underlying idea is to instrument only a subset of the basic blocks of a
function and use the function's control flow graph (CFG) to calculate execution counts for the rest. We can use the same idea for our
purposes. Work profiling and basic block profiling are similar, both need to count how many times each basic block was executed. The only
important difference is that we do not have to store the execution count of each basic block, only the total work.

Graph theory shows that we can calculate precisely the frequencies of all edges between basic blocks if we choose a spanning tree of the
function CFG and instrument the edges in the complement of the spanning tree~\cite{nahapetian73,forman81}.
To make the instrumentation probe placement optimal, we choose the maximum spanning tree,
with the edge weights representing edge frequency estimates. By doing so, we leave as many high frequency edges as possible in the
maximum spanning tree and they will not be instrumented, allowing us to minimize the instrumentation overhead~\cite{forman81,ball94}.

\begin{figure}[t]
\centering {
  \includegraphics[scale=0.8]{figs/cfg-example.pdf}\\\vspace{1ex}
  \resizebox{0.45\textwidth}{!}{
  %\scalebox{0.8}{
     \begin{minipage}{0.5\textwidth}
     Instrumented value for each probe $P_i$:
     \begin{align*}
     \omega(P_0) &= w(B_0) + w(B_1) + w(B_4) + w(B_6) + w(B_7) + w(B_8) + w(B_9)\\
     \omega(P_1) &= w(B_2) - w(B_1) - w(B_4) - w(B_6) - w(B_7) - w(B_8)\\
     \omega(P_2) &= w(B_5) - w(B_6)\\
     \omega(P_3) &= w(B_3) - w(B_4) - w(B_6) - w(B_7)
     \end{align*}
     \end{minipage}
  }
}
  \caption{Example of a CFG with its maximum spanning tree in black. Basic blocks and edges highlighted in red are instrumented.
    Instrumenting them is enough for calculating the total work performed by the whole CFG.}
  \label{fig:cfg-example}
\end{figure}

In contrast to the na\"ive instrumentation where each basic block records only its own amount of work, with the optimal profiling the work
counter increments need to take other basic blocks into account. Figure~\ref{fig:cfg-example} shows an example CFG, where the instrumented
blocks are marked with red. In this example, reaching probe $P_2$ implies not only executing block $B_5$ but also not executing $B_6$.

To compute the amount of work associated
with each instrumentation probe, we first need to determine how reaching a probe relates with executing or not a basic block. Starting from
instrumented edges, we build symbolic expressions for each edge of their frequency counts as a linear function of probe counts, as seen in
Figure~\ref{fig:cfg-example}. If a symbolic expression for an edge and the corresponding block contains a positive term for a probe, this
means that the block is executed when we reach the probe, so the block's cost should be added at that probe. Conversely, if the expression
contains a negative term for a probe, then the block is executed when we do not reach the probe, so the block's cost should be subtracted.
In our example CFG, the symbolic expression for the basic block $B_8$ is $P_0 - P_1$, so the amount of work of $B_8$, denoted by $w(B_8)$,
is incremented in probe $P_0$ and decremented in $P_1$.

\paragraph{Populating the edge flows:}
Intuitively, if all the edge flows are known for the complement of a spanning tree then at any leaf of the spanning tree there is only one
unknown edge flow. This unknown edge flow can be calculated by Kirchhoff's first law\cite{knuth73,ball94},
also known as the principle of \textit{conservation of flow}, which states that
the amount of flow into a vertex equals the amount of output flow.
This process repeats as a bottom-up propagation until all the unknown edge flows have been calculated. This algorithm can be formally defined as a post-order traversal on the
spanning tree. Let $G$ be the CFG. This algorithm first initializes the edge flows $D_{(u,v)}$, for each edge $(u,v)$ in the CFG, as
follows:
\[
D_{(u,v)} \gets
\begin{cases}
    P_{(u,v)} & \quad \text{if edge $(u,v)$ has a probe $P_{(u,v)}$}\\
    0       & \quad \text{otherwise}
\end{cases}
\]
This initialization represents only the trivially known edge flows, which are the
probes themselves (the highlighted edges in red).

Afterwards, the algorithm propagates the known edge flows, inferring
the symbolic expressions in the unknown edge flows.
This is achieved by performing a bottom-up propagation from the trivially known
edge flows (in red) by applying the Kirchhoff's first law, i.e.,
for each vertex with a single unknown edge flow, we compute the difference between
the known incoming edge flows by the known outgoing edge flows.
Formally,
for each vertex $u\in G$, following a post-order traversal of the spanning tree,
the Kirchhoff's first law is applied as the following symbolic operations:
First, sum all the out-going edge flows
\[
%\Sigma^+_u = \sum_{v\in N^+(u)} D_{(u,v)}
\mathrm{SumOut}(u): \sum_{v\in N^+(u)} D_{(u,v)}
\]
and then sum all the in-coming edge flows
\[
%\Sigma^-_u = \sum_{v\in N^-(u)} D_{(v,u)}
\mathrm{SumIn}(u): \sum_{v\in N^-(u)} D_{(v,u)}
\]
For all out-going neighbors $v\in N^+(u)$,
\[
%\forall v\in N^+(u):  D_{(u,v)} \gets
D_{(u,v)} \gets
\begin{cases}
    \mathrm{SumIn}(u) - \mathrm{SumOut}(u) & \!\!\!\quad \text{if $D_{(u,v)} = 0$}\\
    D_{(u,v)}       & \!\!\!\quad \text{otherwise}
\end{cases}
\]
For all in-coming neighbors $v\in N^-(u)$,
\[
%\forall v\in N^-(u):  D_{(v,u)} \gets
D_{(v,u)} \gets
\begin{cases}
    \mathrm{SumOut}(u) - \mathrm{SumIn}(u) & \!\!\!\quad \text{if $D_{(v,u)} = 0$}\\
    D_{(v,u)}  & \!\!\!\quad \text{otherwise}
\end{cases}
\]

\paragraph{Composing the aggregated value of the probes:}
If $x$ is a symbolic expression, then $\kappa_P(x)$ represents the coefficient
of the term $P$ in $x$.
In our case, $\kappa_P(x)$ will belong to the set $\{-1,0,1\}$.
Once all the edge flows are known, then,
for all probes $P$, the amount of work associated
with probe $P$ is:
\[
%\omega(P) = \sum_{u\in G} \kappa_P(\Sigma^+_u)w(u)
\omega(P) = \sum_{u\in G} \kappa_P(\mathrm{SumOut}(u))w(u)
\]
It would be equally valid to define $\omega(P)$ in terms of $\mathrm{SumIn}(u)$
instead of $\mathrm{SumOut}(u)$,
since these values are equal after populating all edge flows,
as it is enforced by the Kirchhoff's law.

\subsection{Relaxed Instrumentation}

Optimal instrumentation significantly reduces the profiling overhead when compared to na\"ive instrumentation, from an average overhead of
79\% to 13\%. In some extreme cases, optimal instrumentation can result in overheads of up to 60\% (see benchmark \texttt{adpcm\_d} in
Figure~\ref{fig:overhead-O3}). To further reduce the overhead in these cases, we propose two relaxation strategies that offer a trade-off
between accuracy and overhead. Our insight was that if the work associated with a probe is small, then ignoring the probe will have little
effect on the total work estimation. By removing such probes, especially if they are frequently executed, we can reduce the instrumentation
overhead dramatically. To contrast with optimal profiling alone, while the optimal placement of probes tries to reduce overhead by placing
probes in edges that are less likely to be executed, the relaxation strategies focus on removing probes that are more likely to be executed
but add little to the work metric.

To select which probes to remove we consider the probes in a control flow region. The saving for removing a probe is the frequency of the block the probe is in. Removing a probe adds some error term. We solve the 0-1 Knapsack optimization problem, removing probes to maximize the total savings while keeping the accumulated error below a threshold:
\begin{gather*}
    \textrm{max.}\quad\sum_{i=0}^{k} f(P_i)x_i,\quad
    \textrm{s.t.}\quad\sum_{i=0}^{k} \varepsilon(P_i)x_i \leq M \\
    x_i\in\{0,1\}, i\in\{0,\ldots,k\}
\end{gather*}
where the set of probes in the region being considered is $\{P_0, P_1, \ldots, P_k\}$, $f(P_i)$ is the execution frequency of probe $P_i$ and $x_i$ denotes the probes selected for removal, $\varepsilon(P_i)$ is the error for removing $P_i$ and $M$ is the maximum error threshold.

For our experiments, we implemented two solvers for the 0-1 Knapsack problems: an optimal but potentially slow brute-force solver and a
greedy heuristic based on sorting the items~\citep{dantzig57}. We use the brute-force solver with a small number of probes, less than
20, and the greedy heuristic otherwise.

We have two relaxation strategies which differ in the control flow regions they tackle and how they calculate the error terms. The first is
a more conservative approach that calculates induced error from removing a probe by considering the worst case of any execution path
through the program. The second is a more aggressive approach that computes induced error based on the relative change of the work
measurement that removing a probe would make to the total amount work for the whole program. We discuss each one of them in detail.

\subsubsection{\WCRelaxTitle Relaxation}

In order to guarantee the upper bound for the error, this conservative approach
decomposes the CFGs of the program into \textit{directed acyclic graphs} (DAGs).
Then, it remove probes within each DAG while
keeping the total error per DAG bounded by the threshold.

By constraining the percentage error of every DAG below a given threshold,
it is guaranteed that the dynamic error of the work metric
will always be bounded by the threshold.
This constraint is ensured by computing the error of removing each probe relative
to the path with the minimum amount of work possible through the DAG.

The \WCRelaxLower relaxation starts by extracting DAGs from the CFG.
First, the algorithm extracts all the subgraphs that represent a loop or the outer most region of the function.
Afterwards, these subgraphs are transformed into DAGs by ignoring the backedge and also by considering that any loop within the subgraph is never executed, i.e., only the headers of the inner loops are actually included into the DAG
An example of this is shown in Figure~\ref{fig:cfg-relax-example}, where the CFG is partitioned into two DAGs.

\begin{figure}[t]
  \centering
  \includegraphics[scale=0.8]{figs/cfg-relax-example.pdf}
  \caption{Example of a CFG containing a loop and its decomposition into DAGs.
           The DAGs are the subgraphs within the dashed boundaries.}
  \vspace{-4mm}
  \label{fig:cfg-relax-example}
\end{figure}

Finally, the error term is determined by calculating $m$, the minimum amount of work that can be performed in the
DAG. Given this, the worst case relative error caused by removing a probe $P_i$ is $\varepsilon(P_i) = \frac{\omega(P_i)}{m}$.
By constraining the worst case
relative error in every single DAG of the CFG, we guarantee that the final error introduced by relaxation will always be below the chosen
threshold.


\subsubsection{\WPRelaxTitle Relaxation}

The \WCRelaxLower relaxation provides hard guarantees that error will remain below the chosen threshold but in many cases these
guarantees are too conservative. It assumes that each loop will always execute the path with the minimum amount of work
possible, which might be orders of magnitude less than the work actually performed. In such cases, we remove too few probes and have little
effect on the profiling overhead, while the actual relative error in our work estimation is negligible. To avoid this problem, we
propose a more aggressive approach that operates on the probes from the whole program at once.

The \WPRelaxLower relaxation works by using block-frequency profiling from previous executions. With this information, we are able to
compute the error of removing a given probe in terms of the whole program's execution, and then select a subset of all the probes to be
removed. $\varepsilon(P_i)$ is the error of removing probe $P_i$ relative to the total work.

Contrary to the per DAG approach, the error introduced by \WPRelaxLower relaxation is not guaranteed to be bounded by the threshold $M$.
Still, we expect it to remain close to the threshold as long as the program's behavior does not deviate significantly from the behavior
captured in profile we used.
