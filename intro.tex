\section{Introduction}

    Selecting good optimization settings is very hard in modern compilers due to the huge number of parameters and the difficulty of
    predicting their interaction with each other, a given program, and the platform. As a result, the default heuristics usually fail to
    choose the best settings. Programs compiled with \texttt{-O3} might be 50\% slower or worse than those compiled with the best
    settings~\cite{fursin07,chen12b}. Iterative, or adaptive, compilation searches for the best compilation settings. Program versions are
    run on a set of representative inputs and compared by their runtimes. This simple technique is highly successful in improving the
    performance of programs. However, selecting a representative input set is difficult because users may not use the program as the
    developer expects and their usage patterns may change unpredictably over time. An optimization which is good for one set of inputs, may
    not be good for another. If the developer guesses the input set wrong, then the wrong optimizations may be chosen, degrading
    performance and energy consumption.

    An adaptive, online technique would solve this problem as the search would use actual, live inputs and the developer is no longer
    responsible for predicting usage patterns. Unfortunately, we cannot compare two different program versions by runtime when each one
    executes a different input entailing different amounts of work. It also may not always be possible to run the inputs through both
    versions due to possible side effects or overheads for replaying executions. Repeating a bank transfer, for example, is undesirable.
    Instead, we can prefer programs which do more work per unit of time. This work metric could be program
    specific~\cite{alameldeen06,coppa14}, e.g. number of database rows processed. However, if the developer is left with the burden of
    manually choosing the best work metric, it might not always be so easy to reason about, and would be better if it could be computed
    automatically and in a more general way. This paper contributes such an automated system to estimate the amount of work, and then
    applies it to online {\itercomp}.

    We take the simple view that the amount of work performed by a program's execution is proportional to the time it takes for its
    unoptimized version to process the input. We believe that this also matches the developer's intentions and expectations of program
    work. We instrument the program, before optimization, to record how much work it does. The work done for each basic block in the
    program is calculated by a cost model of the expected execution time for each instruction. This simple method rather effectively
    estimates the unoptimized execution time. Using this metric for online {\itercomp} and running each input only once is nearly as good
    as using an oracle which can cheat and get the true runtime of the unoptimized program version for any input.

    However, the instrumentation overheads must be kept low to avoid upsetting the user and to reduce the risk of changing how
    optimizations behave relative to each other~\cite{forman81,anderson97,duesterwald00}. To reduce the profiling overhead, in addition to
    performing an optimal placement of the instrumentation probes~\cite{knuth73,forman81,ball94}, we propose a novel probe removal
    strategy, which we call probe \textit{relaxation}, that provides a trade-off between the measurement accuracy and overhead. Each probe
    has an expected cost, based on its execution frequency, and we can calculate the error that removing it will cause in the work metric.
    We solve an optimization problem, maximizing the savings from removed probes constrained by a threshold for the relative induced error.

    We have two versions of probe relaxation. The first is conservative and calculates induced error from removing a probe by considering
    the worst case of any execution path through the program. This gives strong guarantees about the maximum error, but is sometimes so
    conservative that few probes are actually removed and the observed errors are far below the threshold. The second is more aggressive
    and computes induced error based on the relative change of the work measurement that removing a probe would make to the total amount
    work for the whole program. This approach removes more probes and still seems to keep the error below the threshold although it is no
    longer guaranteed to do so. In both cases, the user sees reduced overhead, but the iterative compilation still performs well.

    Our evaluation shows that performing online iterative compilation guided by work efficiency achieves about 80\% of the speedup
    available to an offline oracle, with improvements of up to about 20\% over the compiler's standard optimization. While our solution
    executes each input a single time to measure the work efficiency metric, the offline oracle is allowed to execute multiple times to
    have a statistically sound measurement of the actual speedup. Regarding the two relaxation algorithms, our results show that the
    conservative relaxation reduces the overhead by 30\% on average and the aggressive relaxation reduces the overhead by an average of
    55\% and up to 80\% over the unrelaxed work profiling while often incurring much less than 1\% of dynamic error in the measurement of
    work.

    Our main contributions are:

    \begin{itemize}[leftmargin=3mm]

        \item We propose a work efficiency metric which enables us to compare different optimized versions while running different inputs
        only once.

        \item We show the effectiveness of the work efficiency for guiding {\itercomp} in an online scenario, where the program is expected
        to execute only once for distinct inputs.

        \item A conservative relaxation algorithm is proposed for reducing the overhead of the work profiling, with a guaranteed bound for
        the error.

        \item A more aggressive relaxation algorithm is proposed, which works on the whole program, in order to further reduce the overhead
        of the work profiling.
    \end{itemize}
