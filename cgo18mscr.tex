\PassOptionsToPackage{table}{xcolor}

\documentclass[sigplan,10pt]{acmart}
\input{premable}


\begin{document}
\title{Online {\IterComp}}

\author{  }

%\author{Rodrigo C. O. Rocha}
%\affiliation{%
%  \institution{University of Edinburgh, UK}
%}
%\email{r.rocha@ed.ac.uk}

%\author{Pavlos Petoumenos}
%\affiliation{%
%  \institution{University of Edinburgh, UK}
%}
%\email{ppetoume@inf.ed.ac.uk}

%\author{Lu\'is F. W. G\'oes}
%\affiliation{%
%  \institution{PUC Minas, Brazil}
%}
%\email{lfwgoes@pucminas.br}

%\author{Murray Cole}
%\affiliation{
%  \institution{University of Edinburgh, UK}
%}
%\email{mic@inf.ed.ac.uk}

%\author{Zheng Wang}
%\affiliation{%
%  \institution{Lancaster University, UK}
%}
%\email{z.wang@lancaster.ac.uk}

%\author{Hugh Leather}
%\affiliation{%
%  \institution{University of Edinburgh, UK}
%}
%\email{hleather@inf.ed.ac.uk}

%\renewcommand{\shortauthors}{R. Rocha et al.}

\input{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below.
%
%\begin{CCSXML}
%<ccs2012>
%<concept>
%<concept_id>10011007.10011006.10011041</concept_id>
%<concept_desc>Software and its engineering~Compilers</concept_desc>
%<concept_significance>500</concept_significance>
%</concept>
%<concept>
%<concept_id>10011007.10011006.10011041.10011043</concept_id>
%<concept_desc>Software and its engineering~Retargetable compilers</concept_desc>
%<concept_significance>500</concept_significance>
%</concept>
%</ccs2012>
%\end{CCSXML}

%\ccsdesc[500]{Software and its engineering~Compilers}
%\ccsdesc[500]{Software and its engineering~Retargetable compilers}
\keywords{{\IterComp}, Profiling} %, Relaxed Instrumentation}

\maketitle
\vspace{-1em}

\input{intro}
\input{motivation}

\section{Work Efficiency Metric} \label{sec:metric}

In this section we define work efficiency, a metric proposed for comparing
different optimized versions of a program when executing with different inputs.
We define it as the ratio between the amount of \textit{work}, $\Delta W$,
performed during a period of time, $\Delta t$.
\[
   P = \frac{\Delta W}{\Delta t}
\]

By measuring the amount of \textit{computational work} done per unit of time,
it reduces the impact of input-dependent aspects and focus instead on the
efficiency of the optimized program.
For this metric, the main challenge is to precisely define what represents
\textit{work}.
In the context of experimental algorithmics, previous works have suggested using
\textit{block frequency} as a metric for empirical computational
complexity~\cite{goldsmith07}.
%Block frequency is a relative metric that represents the number of times a basic
%block executes~\citep{ball94,ball96}.

However, in the context of comparing different optimization sequences,
although block frequency would be able to capture aspects of optimizations that
simplify the control-flow graph (CFG), measuring work at the basic block
resolution would not capture effects of optimisations at the instruction level.
%Because of that, in this thesis, we extend the idea of using basic block
%frequency to measure computational work by also considering the computational
%cost of each basic block.
Because of that, we adopt a work metric that takes into account the
computational cost within each basic block.
The computational cost of a basic block is given by weighting the instructions
that it contains.

To this end, we model the computational work $\Delta W$ as a linear equation based on block frequency information and a cost model of the instruction set.
Formally,
\[
\Delta W = \varepsilon + \sum_{B} w(B)f(B)
\]
where $f(B)$ represents the frequency of basic block $B$ and $w(B)$ represents the computational work of executing $B$.
We define the work of a basic block $B$ as the sum of the cost of its instructions, i.e.,
\[
w(B) = \sum_{i} w_i N_B(i)
\]
where $w_i$ is the cost of instruction $i$ and $N_B(i)$ is the number of occurrences of instruction $i$ in basic block $B$.

In this simplified model, we consider that $w_i$ is constant across all programs and executions, varying only between target architectures.
On the other hand, $N_B(i)$ is a program-dependent static value which is known at compile-time and $f(B)$ is a dynamic value known only at run-time,
since $f(B)$ is both program and execution dependent as the execution frequency of a basic block can change when executing with different inputs.

\subsection{Estimating a Cost Model of the Instructions}

Similarly to previous work~\citep{giusto01,powell09,brandolese11}, we derive the cost model of the instruction set by modelling the problem as a multi-variable linear regression, where the \textit{regression coefficients} are the costs of the instructions and the \textit{regressors} are computed as $\sum_B N_B(i)f(B)$ for each instruction.
\begin{equation}\label{eq:linear-work-expression}
\Delta W = \varepsilon + \sum_{i} \left(w_i \sum_{B} N_B(i)f(B)\right)
\end{equation}
By having some empirical data after executing several benchmarks with different inputs, we can fit the linear model with this empirical data in order to obtain estimate costs of the instructions.
In order to fit the linear model, we measure the wall-clock time when executing the training benchmarks described in Section\ref{sec:benchmarks} with their respective 1000 input datasets.
For these measurements, the training benchmarks are compiled without optimization.
The reason for using no optimization, as discussed in Section\ref{sec:oic-infra}, is because the amount of work must be only input dependent and consistent between different optimization sequences.
This aspect is crucial for the work-based performance metric to enable a consistent comparison between the various optimization sequences when executing with distinct inputs.
%it allows to use the estimated cost model for computing a work metric which is independent of optimizations, as we explained in Section~\ref{}.

Our cost model has a total of 52 LLVM instructions\footnote{We do not model all LLVM instructions because some instructions are more common in optimized programs, such as the vector instructions.}.
Every program, in the set of training benchmarks, was compiled twice: once including the necessary instrumentation for the block frequency profiling, which is required for deriving the linear expression defined in Equation~\ref{eq:linear-work-expression};
and a standard compilation without any optimization or instrumentation.
For each input in the training dataset, the benchmarks were executed once with the instrumented version for collecting the block frequency profiling, and multiple times with the standard compilation just for measuring the wall-clock execution time, until the confidence interval was no larger than 1\% for a 99\% confidence.
After collecting these measurements, this data can be used to estimate the unknown parameters in the linear model.
Because we fit the linear model based on the wall-clock execution time, the derived cost model can be interpreted as an estimate of the execution time when the program is compiled without optimizations.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.6\linewidth]{figs/cost-model.pdf}
    \caption{Linear model fitted from empirical data. The mean absolute error (MAE) for the fitted curve is seven milliseconds.}
    \label{fig:cost-model}
\end{figure}

Figure~\ref{fig:cost-model} compare the work metric with the corresponding execution time for some instances of the test benchmarks.
Notice how the fitted model has a higher relative error for the instances with very short execution time, namely those that run for less than one tenth of a second.
The mean absolute error (MAE) for the fitted curve is seven milliseconds.

\subsection{Comparison with Instructions Per Cycle} \label{sec:ipc-vs-work-metric}

The IPC metric has been widely used for studying performance benefits of hardware optimizations.
Although previous work has suggested the use of IPC for guiding {\itercomp}, in this section we argue in favour of the WP metric over IPC.

The IPC metric differs from the proposed WP metric in a key aspect:
the IPC metric is computed solely based on the final optimized program.
When executing different optimized versions of a program with the same input, both the number of instructions and the number of clock cycles can change.
For this reason, higher IPC does not necessarily translate to shorter execution time (or even fewer clock cycles).

% We can illustrate this fact with a very small example as shown in Table~\ref{tab:ipc-example}.
% This example shows two versions of a program, namely P1 and P2, and their respective measurements related to IPC.
% Although version P1 has twice the IPC of P2, P1 is one cycle slower than P2.
% As this example illustrates, the IPC metric can be misleading when compared different versions of the same program.
% This problem is only intensified when both the optimization sequence and the input changes.
% In Chapter~\ref{chap:eval} we show empirical evidence for this argument.
%
% \begin{table}[h]
% \centering
% \begin{tabular}{|c|c|c|c|}
% \hline
%                        & P1 & P2  \\
% \hline
% Number of Instructions & 5  & 2   \\
% Number of Cycles       & 5  & 4   \\
% IPC                    & 1  & 0.5 \\
% \hline
% \end{tabular}
% \caption{Example that illustrates that a higher IPC does not necessarily translate to shorter execution time.}
% \label{tab:ipc-example}
% \end{table}

On the other hand, WP computes the amount of work based on the unoptimized version of the program, which means that it always measures the same amount of work for the same input, regardless of the optimizations applied on the program.
This is a key aspect that enables the use of the WP metric for guiding {\itercomp}, because a higher WP, which represents work per unit time, naturally translate to shorter execution time.
Moreover, previous work has also presented other arguments against the use of IPC in similar use-case scenarios. %, as discussed in Chapter~\ref{chap:related}.

\section{Online {\IterComp} Infrastructure} \label{sec:oic-infra}

Although {\itercomp} had been originally proposed as an \textit{offline} optimization strategy, it can also be adapted to work in online scenarios.
Instead of selecting the best optimization sequence as part of the development time (pre-shipping) of a program, a first version of the program is shipped together with an {\itercomp} mechanism.
In the online scenario, the program is shipped with an initial optimization sequence and different optimization sequences are evaluated as the end-user executes the program.
This optimization strategy is also known as idle-time optimization, as the re-compilation happens between runs of the program.

LLVM is particularly suitable for iterative compilation as it makes possible to cache a pre-compiled, but still unoptimized, version of the input program in the bitcode format of the LLVM IR.
This caching allows to speedup the time required for re-compilation as it is able to bypass the frontend phase.
If re-compilation time is critical, it would also be possible to keep only the hot portion of the code in the LLVM bitcode format, while the remaining portion of the code is already compiled to the final object code.
However, this is out of the scope of this paper and we always re-compile the whole program.

\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{figs/infra-diagram}
    \caption{Overview of the execution engine for applying {\itercomp}.}
    \label{fig:infra-diagram}
\end{figure}
%\textbf{Describe the process step by step.}

Figure~\ref{fig:infra-diagram} shows an overview of the infrastructure required for applying online {\itercomp}
using WP as the metric of choice for evaluating optimization sequences.
The online {\itercomp} follows as described bellow:
\begin{enumerate}
\item The program is pre-compiled to the LLVM bitcode format without optimization.
\item The unoptimized program is instrumented for work profiling.
\item Execution-based optimization search:
 \begin{enumerate}
   \item The current optimization sequence is used for the re-compilation of the program.
   \item The program is executed with any input provided by the end-user.
         During the execution of the program, wall-clock time and the work metric are recorded by profiling instrumentation.
   \item If the recorded profiling for the current optimization can be used to compute an average performance measurement within a small confidence interval,
         then a new optimization sequence is generated.
         Otherwise, the same optimization sequence is used for the next execution.
 \end{enumerate}
\end{enumerate}

In this work, we focus mainly on the highlighted components.
The \textit{Work Instrumentation} phase focuses on providing a low-overhead instrumentation for profiling the work metric.
The instrumentation consists of adding a global counter to the program which is used to accumulate the amount of work computed during the program's execution, using the cost model of the instruction set.
A detailed description of the work instrumentation is presented in Section~\ref{chap:instr}.

Notice how the \textit{Work Instrumentation} phase is executed before performing any optimization to the program.
It is critical so that the work profiling always measure the same amount of work for a given input, regardless of the optimization sequence applied to the program.
Because the instrumentation is performed before optimising the program, it means that the work profiling derives the linear expression defined in Equation~\ref{eq:linear-work-expression}
based on the unoptimized program.
In other words, the basic blocks and the number of occurrences of the instruction in the basic blocks reflect the unoptimized program.
This particular sequence of compilation guarantees that the amount of work must is only input dependent, but consistent between different optimization sequences.

As the name suggests, the component called \textit{Optimization Selector} is responsible for selecting which optimization sequence to use for the next execution of the program.
It can either keep the same optimization sequence used in the previous execution or start monitoring the performance of a new optimization sequence.
An optimization sequence can be kept for multiple executions of the program in order to gather enough measurement to compute an average performance with small statistical deviations, i.e., for which the confidence interval has a small range.
We call by \textit{Input-Window Size} the number of executions performed using the same optimization sequence.

\subsection{Real Online Scenarios}

In most online scenarios, it is common for periods of peak usage and idle periods.
For example, mobile devices are usually intensely used during the day, with some idle periods usually when the battery is being re-charged~\citep{mpeis16}.
Similarly, many authors have also considered peak and idle (or underutilized) periods in the context of data centres~\citep{armbrust10,chen12b}.

The proposed infrastructure is very well suited for these real online scenarios\footnote{
However, if idle time is almost non-existent, the proposed infrastructure can still be used by re-compiling the program with a different optimization while multiple runs of the program are being executed.}.
In particular, periods of peak usage could be used to monitor multiple runs of the program using the same optimization sequence, collecting the work profiling and measuring its execution time,
while periods of idleness or underutilisation could be leveraged to use the profiling statistics for selecting better optimizations and re-compiling the program.

\section{Work Profiling}

In this section, we describe how the computation of the work metric can be performed during runtime by means of instrumenting the code.
%In particular, we adapt the optimal algorithm proposed originally for block-frequency profiling~\citep{nahapetian73,knuth73,ball94}.
Because we define work as a linear equation on the block-frequency counters, it is possible to embed its computation into the execution of the program.
A naive instrumentation would consist basically of having a global counter that starts with the interception value, $\varepsilon$, and each basic block increments its own cost into the global counter.
Although this instrumentation is easily implemented, it imposes a significant overhead on the instrumented program.
However, it is possible to insert fewer probes by carefully placing the probes in a way that is possible to reconstruct the complete profiling information~\citep{knuth73,ball94}.

We adapt the optimal block-frequency profiling in order to perform the work profiling efficiently.
The proposed work profiling differs from the optimal block-frequency profiling as the latter occurs in two stages:
\textit{(i.) Before execution:} The code is instrumented with counters for each probe.
\textit{(ii.) After execution:} The information from the recorded probes is propagated in the CFGs of the program.
In contrast, our work profiling has a single counter, and it only requires the instrumentation before execution, without any post-processing of the recorded profiling.

The instrumented code is assumed to be generated before optimising the code.
This assumption is based on three key points:
\textit{(i.)} it guarantees that the work metric is independent of optimisation, i.e., the same input is always mapped to the same amount of work, regardless of the optimisation;
\textit{(ii.)} it simplifies the code generator;
\textit{(iii.)} it leverages from the optimisations to further improve the instrumentation code.
Having a work metric that is independent of optimisation is the most important reason for which we instrument the code before optimisations.

The optimal placement of the probes works by first computing the maximum spanning
tree based on a weighing that assigns a value to each edge in the CFG.
These weights can be obtained either by empirical measurements or heuristic estimations,
and their goal is to avoid probing in frequently executed edges.
Once we have the maximum spanning tree, probes are placed on every edge not in the spanning tree.
Figure~\ref{fig:cfg-example} shows an example of a CFG with a maximum spanning tree.
%represented by the black edges, while the edges highlighted in red represent the placement of the probes.

\begin{figure}[t]
\centering {
  \includegraphics[scale=0.75]{figs/cfg-example.pdf}\\\vspace{1ex}
  \resizebox{0.45\textwidth}{!}{
  %\scalebox{0.8}{
     \begin{minipage}{0.5\textwidth}
     Instrumented value for each probe $P_i$:
     \begin{align*}
     \omega(P_0) &= w(B_0) + w(B_1) + w(B_4) + w(B_6) + w(B_7) + w(B_8) + w(B_9)\\
     \omega(P_1) &= w(B_2) - w(B_1) - w(B_4) - w(B_6) - w(B_7) - w(B_8)\\
     \omega(P_2) &= w(B_5) - w(B_6)\\
     \omega(P_3) &= w(B_3) - w(B_4) - w(B_6) - w(B_7)
     \end{align*}
     \end{minipage}
  }
}
  \caption{Example of a CFG with its minimum spanning tree in black and the
   basic blocks highlighted in red represent the instrumented basic blocks with
   the placement of the probes.}
  \label{fig:cfg-example}
\end{figure}

In contrast to the naive instrumentation where each basic block records only its
own amount of work, with the optimal profiling, the instrumented basic blocks need
to record an aggregated value of work that also takes other basic blocks into
account.
Some probes speculatively assume a path of execution,
while other probes correct when these assumptions are wrong
(see for example $w(P_0)$ and $w(P_1)$ in Figure~\ref{fig:cfg-example}).

We compute the aggregated value of work for each probe in two steps:
\textit{(i.)} we propagate information about the probes through the edges of the CFG,
as symbolic expressions, and then \textit{(ii.)} we use this edge flows
to compose the aggregated value of the probes.

The symbolic expressions in the edges can then be used to compose the aggregated
value of the probes.
These symbolic expressions describe the probes that are in the same path of the
edge (positive terms) and those that are in complementary paths (negative
terms).
Therefore, the positive terms in the summed symbolic expression of a
basic block indicate that the amount of work of this basic block will be
incremented in the probes represented by these positive terms.
Similarly, the negative terms indicate that the amount of work of this basic block
will be decremented in the probes represented by these negative terms.
For example, because the symbolic expression for the basic block $B_8$ is $P_0 - P_1$,
the amount of work of $B_8$, denoted by $w(B_8)$, is incremented in probe $P_0$
and decremented in $P_1$.

\noindent\textbf{Populating the edge flows:}
Intuitively, if all the edge flows are known for the complement of a spanning tree
then at any leaf of the spanning tree there is only one unknown edge flow.
This unknown edge flow can be calculated by Kirchhoff's first law.
This process repeats as a bottom-up propagation until all the unknown edge flows
have been calculated.
This algorithm can be formally defined as a post-order traversal on the spanning
tree.
Let $G$ be the CFG.
This algorithm first initializes the edge flows $D_{(u,v)}$, for each edge
$(u,v)$ in the CFG, as follows:
\[
D_{(u,v)} \gets
\begin{cases}
    P_{(u,v)} & \quad \text{if edge $(u,v)$ has a probe $P_{(u,v)}$}\\
    0       & \quad \text{otherwise}
\end{cases}
\]
Afterwards, for each vertex $u\in G$, in a post-order traversal of the spanning tree,
we can use the Kirchhoff's first law and apply the following operations
in a symbolic fashion:
\[
\Sigma^+_u = \sum_{v\in N^+(u)} D_{(u,v)}
\]
\[
\Sigma^-_u = \sum_{v\in N^-(u)} D_{(v,u)}
\]
\[
\forall v\in N^+(u):  D_{(u,v)} \gets
\begin{cases}
    D_{(u,v)} & \quad \text{if $D_{(u,v)}\neq 0$}\\
    \Sigma^-_u - \Sigma^+_u       & \quad \text{otherwise}
\end{cases}
\]
\[
\forall v\in N^-(u):  D_{(v,u)} \gets
\begin{cases}
    D_{(v,u)} & \quad \text{if $D_{(v,u)}\neq 0$}\\
    \Sigma^+_u - \Sigma^-_u       & \quad \text{otherwise}
\end{cases}
\]

\noindent\textbf{Composing the aggregated value of the probes:}
If $x$ is a symbolic expression, then $\kappa_P(x)$ represents the coefficient
of the term $P$ in $x$.
In our case, $\kappa_P(x)$ will belong to the set $\{-1,0,1\}$.
Then, for all probes $P$:
\[
\omega(P) = \sum_{u\in G} \kappa_P(\Sigma^+_u)w(u)
\]

\subsection{Relaxed Instrumentation}

Although the optimal instrumentation significantly reduces the profiling
overhead when compared to the naive instrumentation, from an average overhead
of 79\% to 13\%, in some critical cases, even the optimal instrumentation can
result in overheads of up to 60\% (see benchmark \texttt{adpcm\_d} in Figure~\ref{fig:overhead-O3}).
In order to further reduce the overhead in these critical cases, we propose a
relaxation strategy that offers a trade-off between accuracy and overhead.
While the optimal instrumentation places probes in edges that are
less likely to be executed, the relaxation removes probes that are
more likely to be executed but add little to the final work metric.

% \begin{figure}[h]
%   \centering
%   \includegraphics[scale=0.85]{figs/relax-instr-diagram.pdf}
%   \caption{Overview of the work instrumentation algorithm, including the relaxation technique.}
%   \label{fig:relax-instr-diagram}
% \end{figure}

%Figure~\ref{fig:relax-instr-diagram} shows an overview of the relaxed instrumentation algorithm.
%The highlighted step is introduced by the relaxed instrumentation on top of the previously defined optimal profiling.
The relaxation strategy performs a post processing on the resulting instrumentation of the optimal algorithm.
This post processing identifies probes that add little to the work metric, and removes their instrumentation.
In order to guarantee an upper bound for the dynamic error in the profiling measurement,
the relaxation algorithm applies a constrained post-processing on a per DAG (directed acyclic graph) basis.
By constraining the relaxation within each DAG by a maximum allowed static error,
it guarantees that the overall relaxation will also be constrained by the same bound.

The relaxation starts by extracting DAGs from the CFG.
First, the algorithm extracts all the subgraphs that represent a loop or the outer most region of the function.
Afterwards, these subgraphs are transformed into DAGs by ignoring the backedge
and also by considering that any loop within the subgraph is never executed,
i.e., only the headers of the inner loops are actually included into the DAG.
Figure~\ref{fig:cfg-relax-example} shows a CFG partitioned into two DAGs (consider
only basic blocks and edges completely inside the yellow and green boundaries).

\begin{figure}[h]
  \centering
  \includegraphics[scale=0.75]{figs/cfg-relax-example.pdf}
  \caption{Example of a CFG containing a loop and its decomposition into DAGs.
           The DAGs are the subgraphs within the dashed boundaries.}
  \label{fig:cfg-relax-example}
\end{figure}

%\begin{lstlisting}[caption={Optimal placement of probes for block frequency.}, label={lst:instrumentCFG}]
%// Input: CFG
%relaxInstrumentation(G) {
%  for loop in G:
%     DAG = colapseInnerLoops(loop)
%     relaxInstrumentedDAG(DAG)
%  DAG = colapseInnerLoops(G)
%  relaxInstrumentedDAG(DAG)
%}
%\end{lstlisting}

For every DAG with a set of probes $\{P_0, P_1, \ldots, P_k\}$,
we select a subset of the probes to be removed, subject to
the maximum allowed percentage error, $M$.
This strategy can be modeled as a 0-1 Knapsack problem:
\begin{gather*}
\textrm{max.}\quad\sum_{i=0}^{k} f(P_i)x_i,\quad
\textrm{s.t.}\quad\sum_{i=0}^{k} \varepsilon(P_i)x_i \leq M \\
x_i\in\{0,1\}, i\in\{0,\ldots,k\}
\end{gather*}
where $f(P_i)$ is the execution frequency of probe $P_i$, $x_i$ denotes the
probes selected for removal, and $\varepsilon(P_i)$ is the percentage error of
removing probe $P_i$ relative to the minimum work value possible to compute in
the DAG, i.e., if $m$ is the minimum amount of work possible to be computed when
executing the DAG, then $\varepsilon(P_i) = \frac{\omega(P_i)}{m}$.
Because the percentage error is computed based on the path with the minimum
amount of work, $\varepsilon(P_i)$ represents the maximum error possible that
would be incurred when removing probe $P_i$.
Furthermore, by constraining the percentage error of every DAG below a given
threshold, we guarantee that the final error of the relaxation will always be
bounded by the threshold.
% Furthermore, by constraining the percentage error of every DAG below a given threshold, we guarantee that the final error of the relaxation will always be bounded by the threshold, as demonstrated by Proposition~\ref{prop:relax-bound}.
%
% \begin{prop}\label{prop:relax-bound}
% Let $n_i$ be the number of times a given DAG $i$ is executed, $r_i$ be the total relaxation (amount of work removed) in DAG $i$, and $m_i$ be its minimum amount of work.
% If $\frac{r_i}{m_i} \leq M$ for every $i$,
% then the final error of the relaxation will always be bounded by the same threshold.
% \end{prop}
% \begin{proof}
% We can model the overall error of the relaxation as:
% \[
% 1 - \frac{n_1(m_1 - r_1) + n_2(m_2 - r_2) + \ldots + n_k(m_k - r_k) + c}{n_1m_1 + n_2m_2 + \ldots + n_km_k + c}
% \]
% That is,
% %\begin{equation*}
% \begin{gather*}
%  1 - \frac{n_1m_1 + n_2m_2 + \ldots + n_km_k + c}{n_1m_1 + n_2m_2 + \ldots + n_km_k + c} + \frac{n_1r_1 + n_2r_2 + \ldots + n_kr_k}{n_1m_1 + n_2m_2 + \ldots + n_km_k + c} = \\
%  \frac{n_1r_1 + n_2r_2 + \ldots + n_kr_k}{n_1m_1 + n_2m_2 + \ldots + n_km_k + c}
% \end{gather*}
% %\end{equation*}
% If $\frac{r_j}{m_j}$ is the maximum ratio $\frac{r_i}{m_i}$ for every $i$, then
% \begin{equation*}
% \begin{aligned}
%  \frac{n_1r_1 + n_2r_2 + \ldots + n_kr_k}{n_1m_1 + n_2m_2 + \ldots + n_km_k + c} &\leq\\
%  \frac{n_1r_j + n_2r_j + \ldots + n_kr_j}{n_1m_j + n_2m_j + \ldots + n_km_j + c} &\leq\\
%  \frac{n_1r_j + n_2r_j + \ldots + n_kr_j}{n_1m_j + n_2m_j + \ldots + n_km_j} &
% \end{aligned}
% \end{equation*}
% If $N = max\{n_i$ for every $i\}$, then
% \begin{equation*}
% \begin{aligned}
%  \frac{n_1r_j + n_2r_j + \ldots + n_kr_j}{n_1m_j + n_2m_j + \ldots + n_km_j} &\leq\\
%  \frac{Nr_j + Nr_j + \ldots + Nr_j}{Nm_j + Nm_j + \ldots + Nm_j} &=\\
%  \frac{Nkr_j}{Nkm_j} = \frac{r_j}{m_j} &\leq M
% \end{aligned}
% \end{equation*}
% \end{proof}


%\begin{lstlisting}[caption={Optimal placement of probes for block frequency.}, label={lst:instrumentCFG}]
%// Input: CFG
%relaxInstrumentedDAG(DAG){
%   P = ProbesIn(DAG)
%   m = minWork(DAG,P)
%   K = createKnapsackModel(P,m)
%   Bag = solveKnapsack(K)
%   for B in (P-Bag):
%     removeProbe(B)
%}
%\end{lstlisting}

%The necessary block-frequency information for optimising both the placement of
%probes can be acquired from profiles of previous executions of the program or by
%a static heuristic of the CFG during compilation.

For our experiments, we implemented two solvers for the 0-1 Knapsack problem:
the optimal brute-force solver and
the greedy heuristic based on sorting the items~\citep{dantzig57}.
We use the brute-force solver for DAGs with a small number of probes and the
greedy heuristic when the number of probes is greater than a threshold.
Some of the benchmarks have DAGs with several hundreds of probes, which could
result in a long compilation time.

\subsection{Whole Program Relaxation}

In some cases, the proposed relaxation can be very conservative, because it
considers the static error of removing probe $P_i$ relative to the minimum work
possible of a DAG, in order to be able to guarantee that the dynamic error will
be bounded by a given threshold.
This conservatism can be overly restrictive in some cases, resulting in a
negligible overhead reduction but also causing just a negligible dynamic error
to the work profiling.

In some cases, this overly restrictive conservatism may be unnecessary.
For these cases, we propose an adapted version of the relaxation algorithm that operates on the whole program.
Traditionally, compilers optimizations are performed on the function-level, or at best on a per module basis.
Whole program optimization (WPO) means that the compiler considers all compilation units of the program and optimizes them using the combined knowledge of how they are used together.

The whole program relaxation works by using block-frequency profiling from previous executions.
By having this profiling information, the \textit{whole program relaxation} is able to compute the error of removing a given probe in terms of the whole program's execution,
and then use this error values for selecting a subset of all the probes to be removed.

For a program with a set of probes $\{P_0, P_1, \ldots, P_k\}$, we model the whole program the relaxation as the following 0-1 Knapsack problem:
\begin{gather*}
\textrm{max.}\quad\sum_{i=0}^{k} f(P_i)x_i,\quad
\textrm{s.t.}\quad\sum_{i=0}^{k} \varepsilon(P_i)x_i \leq M \\
x_i\in\{0,1\}, i\in\{0,\ldots,k\}
\end{gather*}
where $f(P_i)$ is the execution frequency of the instrumented basic block $P_i$, $x_i$ denotes the probes selected for removal, $M$ is the error threshold, and $\varepsilon(P_i)$ is the percentage error of removing probe $P_i$ relative to the profiled global work, i.e.,
if $\Delta W$ is the work value for the whole program's execution, computed from the basic block frequencies profiled from previous executions, the error for a given probe $P_i$ is
\[
\varepsilon(P_i) = \frac{\omega(P_i)f(P_i)}{\Delta W}.
\]

Contrary to the per DAG relaxation, the whole program relaxation is not guaranteed to be bounded by the error threshold $M$,
as it depends on the representativity % representativeness
 of the profiling information provided to the whole program relaxation.


\input{setup}

\input{results}

\input{related}

\input{conclusions}

%
%\section*{Acknowledgments}
%
%This work was supported by the UK Engineering
%and Physical Sciences Research Council (EPSRC) under grants
%EP/L01503X/1 for the University of Edinburgh, School
%of Informatics, Centre for Doctoral Training in Pervasive
%Parallelism, % (\url{http://pervasiveparallelism.inf.ed.ac.uk/}),
%and also by the Institute for Computing Systems Architecture (ICSA)
%in the School of Informatics at the University of Edinburgh.

\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography}

\end{document}
