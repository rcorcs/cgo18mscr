\section{Experimental Setup}\label{sec:setup}

\paragraph{Compiler and Hardware.}
Our approach is implemented in LLVM 4.0 and is open sourced\footnote{Download URL hidden for anonymous review.}. Our evaluation platform has
a quad-core 3.4 GHz Intel Core i7 CPU with 16 GB of RAM. The operating system is openSUSE 42.2 with Linux kernel version 4.4.27.

\paragraph{Benchmarks.}
Our work uses the \textit{KDataSets} benchmark suite~\cite{chen10,chen12a}. This suite provides 1,000 different inputs per benchmark. The
inputs have different sizes and often lead to different program execution paths. This benchmark suite allows us to evaluate our approach on
a large number of inputs provided by independent developers. We split the entire suite in two distinct subsets, the training and testing
subsets. The former is shown in Table~\ref{tab:kdatasets:training} and was used for calibrating the work model of
Section~\ref{subsec:workmetric}. The latter is shown in Table~\ref{tab:kdatasets} and was used for evaluating our approach.

\input{table-benchs1}

\paragraph{Measurement Report.}
To reduce the measurement noise, all experimental runs were repeated multiple times until the width of the 95\% confidence interval of the
runtime came within 5\% of the mean runtime. For all reported metrics, our figures show the geometric mean of the metric for each benchmark
across all inputs, as well as a min-max bar representing the variance across inputs.

\paragraph{Optimization Space Search.}

The focus of this paper is on developing a new way for measuring algorithmic efficiency and enabling true online {\itercomp}. The choice of
mechanism for driving the search of the optimization space is orthogonal. We used a straightforward search approach where we exhaustively
search a moderate-sized fixed subset of the full optimization space. This subset contains 500 randomly selected optimization sequences. A
more efficient stochastic search mechanism would produce better results but would also introduce randomness in our results, partially
obfuscating the effect of driving iterative compilation with our work efficiency metric.


%These optimization sequences contain an average of 40 individual optimization passes,
%including repetitions, with a maximum of 119 optimization passes, but it also contains
%optimization sequences which consist of a single flag, such as the default optimizations
%\texttt{-O1}, \texttt{-O2}, \texttt{-O3}, \texttt{-Os}, and \texttt{-Oz}.




%All optimization sequences in the set of optimizations were generated completely
%at random, without using any knowledge of individual transformations.
%Each optimization sequence was generated in two steps: \textit{(1)} randomly
%selects the number of flags; \textit{(2)} randomly selects the flags, allowing repetitions.
%Afterwards, this randomly generated optimization sequence would be included in
%the set of optimization sequences only if it was able to improve the performance
%of a training benchmark, also selected at random, in respect of the \texttt{-O3}
%default optimization.
%This process was repeated until we obtained all the 500 distinct optimization sequences.

  % \begin{minipage}{0.9\textwidth}
  %    \vspace{1em}
  %    \noindent\textbf{Example of a short optimization sequence:}\vspace{-1ex}
  %    \justify{\flagstype -mem2reg -simplifycfg -constprop -dce}
  % \end{minipage}
  %
  % \begin{minipage}{0.9\textwidth}
  %    \vspace{1em}
  %    \noindent\textbf{Example of a long optimization sequence:}\vspace{-1ex}
  %    \justify{\flagstype -globalopt -reassociate -instcombine -loop-rotate -block-freq -deadargelim -early-cse -sroa -argpromotion -sccp -tbaa -barrier -constmerge \mbox{-loop-vectorize} -domtree -basicaa -memdep -basiccg -memcpyopt \mbox{-constprop} -adce -globaldce -mem2reg -constmerge \mbox{-globaldce} -constprop -instsimplify -dse -dce -simplifycfg -loop-unroll -reassociate -constprop \mbox{-globaldce} -instsimplify -adce -constmerge -bb-vectorize -dce -mergefunc -simplifycfg -dse -loop-unroll -globaldce}
  % \end{minipage}
  %
  % \begin{minipage}{0.9\textwidth}
  %    \vspace{1em}
  %    \noindent\textbf{Example of an optimization sequence which includes {-O3}:}\vspace{-1ex}
  %    \justify{\flagstype -O3 -adce -globaldce -simplifycfg -memcpyopt -reassociate -mergefunc \mbox{-dce} -dse}
  %    \vspace{2em}
  % \end{minipage}

%Repeating the same optimization pass can be beneficial and usually expected by other passes.
%For example, the {\flagstype -loop-simplify} pass is used for transforming loops into a canonical form by inserting pre-header and exit basic blocks.
%Although this pass inserts jumps due to redundant basic blocks, this canonical form can be favourable to other loop optimizations.
%Because of the redundant basic blocks, this optimization pass expects that the {\flagstype -simplifycfg} will eventually be executor later on the optimization pipeline.
%Another example of such inter-relation between transformations concerns the {\flagstype -licm} and {\flagstype -mem2reg} passes.
%The {\flagstype -licm} pass is responsible for moving invariant code out from the loop body.
%It usually creates new local variables, using memory access operations, for assisting with the code manipulation, which means that the executing the {\flagstype -mem2reg} pass afterwards would be useful as a cleanup pass for removing the extra memory accesses generated.
%However, many of the analysis required for identifying loop invariant also benefit from the transformations performed by the {\flagstype -mem2reg} pass.
%These examples illustrate the importance of repeating optimization passes.
%Moreover, they illustrate the intricate relation amongst several transformations.
