\section{Experimental Setup}\label{sec:setup}

\paragraph{Compiler and Hardware} We implemented our approach in LLVM 4.0.
We have made it available for download\footnote{Download URL hide for anonymous review.}.
Our evaluation platform has a quad-core 3.4 GHz Intel Core i7 CPU with 16 GB of RAM.
The operating system is openSUSE 42.2 with Linux kernel 4.4.27.

%The target platform is a Linux-4.4.27 system with an Intel Core i7-4770 3.40GHz Skylake~CPU with 16~GiB RAM.

\paragraph{Benchmarks}
We use a subset of the \textit{KDataSets} benchmark suite~\cite{chen10,chen12a},
shown in Table~\ref{tab:kdatasets}.
This suite provides 1,000 different inputs per benchmark.
The inputs have different sizes and often lead to different program execution paths.
%A summary of the benchmarks and their datasets is given in Table~\ref{tab:kdatasets}.
This benchmark suite allows us to evaluate our approach on a large number of inputs provided by independent
developers.

\input{table-benchs1}

%Table~\ref{tab:kdatasets} gives the benchmarks used to train the cost model that is used for computing the instruction weights for
%the work metric. These were also used for collecting a fixed set of optimizations for the {\itercomp}. Table~~\ref{tab:kdatasets:test}
%lists the testing benchmarks used to evaluate our approach.


\paragraph{Performance Report}
We execute each benchmark with an input a number of times until the gap of the upper and lower confidence bounds is smaller than 5\% under
a 95\% confidence interval setting. We report the geometric mean performance across runs and inputs per benchmark, and provide a min-max
bar to show the variance across inputs.


\paragraph{The Set of Optimization Sequences}

For the purpose of evaluating the use of the work efficiency metric for guiding
online {\itercomp}, we collected in advance a fixed set of optimization sequences.
The reason for using this fixed set is to allow a direct comparison among the configurations.
We assume that a good generator of optimization sequences will be used in a real online scenario.
Having a better generator would only improve the whole process of the online {\itercomp}.
This set contains 500 optimization sequences collected in a random search using the training benchmarks.
These optimization sequences contain an average of 40 individual optimization passes,
including repetitions, with a maximum of 119 optimization passes, but it also contains
optimization sequences which consist of a single flag, such as the default optimizations
\texttt{-O1}, \texttt{-O2}, \texttt{-O3}, \texttt{-Os}, and \texttt{-Oz}.

%All optimization sequences in the set of optimizations were generated completely
%at random, without using any knowledge of individual transformations.
%Each optimization sequence was generated in two steps: \textit{(1)} randomly
%selects the number of flags; \textit{(2)} randomly selects the flags, allowing repetitions.
%Afterwards, this randomly generated optimization sequence would be included in
%the set of optimization sequences only if it was able to improve the performance
%of a training benchmark, also selected at random, in respect of the \texttt{-O3}
%default optimization.
%This process was repeated until we obtained all the 500 distinct optimization sequences.

  % \begin{minipage}{0.9\textwidth}
  %    \vspace{1em}
  %    \noindent\textbf{Example of a short optimization sequence:}\vspace{-1ex}
  %    \justify{\flagstype -mem2reg -simplifycfg -constprop -dce}
  % \end{minipage}
  %
  % \begin{minipage}{0.9\textwidth}
  %    \vspace{1em}
  %    \noindent\textbf{Example of a long optimization sequence:}\vspace{-1ex}
  %    \justify{\flagstype -globalopt -reassociate -instcombine -loop-rotate -block-freq -deadargelim -early-cse -sroa -argpromotion -sccp -tbaa -barrier -constmerge \mbox{-loop-vectorize} -domtree -basicaa -memdep -basiccg -memcpyopt \mbox{-constprop} -adce -globaldce -mem2reg -constmerge \mbox{-globaldce} -constprop -instsimplify -dse -dce -simplifycfg -loop-unroll -reassociate -constprop \mbox{-globaldce} -instsimplify -adce -constmerge -bb-vectorize -dce -mergefunc -simplifycfg -dse -loop-unroll -globaldce}
  % \end{minipage}
  %
  % \begin{minipage}{0.9\textwidth}
  %    \vspace{1em}
  %    \noindent\textbf{Example of an optimization sequence which includes {-O3}:}\vspace{-1ex}
  %    \justify{\flagstype -O3 -adce -globaldce -simplifycfg -memcpyopt -reassociate -mergefunc \mbox{-dce} -dse}
  %    \vspace{2em}
  % \end{minipage}

%Repeating the same optimization pass can be beneficial and usually expected by other passes.
%For example, the {\flagstype -loop-simplify} pass is used for transforming loops into a canonical form by inserting pre-header and exit basic blocks.
%Although this pass inserts jumps due to redundant basic blocks, this canonical form can be favourable to other loop optimizations.
%Because of the redundant basic blocks, this optimization pass expects that the {\flagstype -simplifycfg} will eventually be executor later on the optimization pipeline.
%Another example of such inter-relation between transformations concerns the {\flagstype -licm} and {\flagstype -mem2reg} passes.
%The {\flagstype -licm} pass is responsible for moving invariant code out from the loop body.
%It usually creates new local variables, using memory access operations, for assisting with the code manipulation, which means that the executing the {\flagstype -mem2reg} pass afterwards would be useful as a cleanup pass for removing the extra memory accesses generated.
%However, many of the analysis required for identifying loop invariant also benefit from the transformations performed by the {\flagstype -mem2reg} pass.
%These examples illustrate the importance of repeating optimization passes.
%Moreover, they illustrate the intricate relation amongst several transformations.
